{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uWMOW336hUz",
        "outputId": "db443321-a596-4972-ab5e-f3cfc4c35834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Target names: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "First 10 rows of X:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "print(\"Feature names:\", feature_names)\n",
        "print(\"Target names:\", target_names)\n",
        "print(\"\\nFirst 10 rows of X:\\n\", X[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SPLITING THE DATASET INTO TRAINING AND TESTING\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size = 0.3, random_state = 1\n",
        ")\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxAKdE5BJKyp",
        "outputId": "c68c6139-5a05-4ec4-bff3-1235272e878c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105, 4)\n",
            "(45, 4)\n",
            "(105,)\n",
            "(45,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN THE MODEL\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size = 0.4, random_state=1\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "classifier_knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "classifier_knn.fit(X_train, y_train)\n",
        "y_pred = classifier_knn.predict(X_test)\n",
        "# Finding accuracy by comparing actual response values(y_test)with predicted response value(y_pred)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "# Providing sample data and the model will make prediction out of that data\n",
        "\n",
        "sample = [[5, 5, 3, 2], [2, 4, 3, 5]]\n",
        "preds = classifier_knn.predict(sample)\n",
        "pred_species = [iris.target_names[p] for p in preds] \n",
        "print(\"Predictions:\", pred_species)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBo65Zb9J5Dd",
        "outputId": "e67311b3-4005-435d-bbd3-d1293bf13374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9833333333333333\n",
            "Predictions: ['versicolor', 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Once you train the model, it is desirable that the model should be persist for future use so that we do not need to retrain it again and again. It can be done with the help of dump and load features of joblib package.\n",
        "\n",
        "import joblib\n",
        "joblib.dump(classifier_knn, 'iris_classifier_knn.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM26JYrTLEaW",
        "outputId": "333da5b0-d1de-41f0-f0e0-cc13c94ef451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iris_classifier_knn.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The above code will save the model into file named iris_classifier_knn.joblib. Now, the object can be reloaded from the file with the help of following code âˆ’\n",
        "\n",
        "joblib.load('iris_classifier_knn.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTLSQjA2LlnH",
        "outputId": "bb1c58df-8899-492f-bc5f-98f79370469b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "Input_data = np.array([\n",
        "   [2.1, -1.9, 5.5],\n",
        "   [-1.5, 2.4, 3.5],\n",
        "   [0.5, -7.9, 5.6],\n",
        "   [5.9, 2.3, -5.8]]\n",
        ")\n",
        "data_binarized = preprocessing.Binarizer(threshold=0.5).transform(Input_data)\n",
        "print(\"\\nBinarized data:\\n\", data_binarized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKB3jTYULxPy",
        "outputId": "c6e8bb8a-0419-4860-f3b4-33e8b7f5a26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Binarized data:\n",
            " [[1. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Mean Removal: This technique is used to eliminate the mean from feature vector so that every feature centered on zero.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "Input_data = np.array([\n",
        "   [2.1, -1.9, 5.5],\n",
        "   [-1.5, 2.4, 3.5],\n",
        "   [0.5, -7.9, 5.6],\n",
        "   [5.9, 2.3, -5.8]]\n",
        ")\n",
        "\n",
        "#displaying the mean and the standard deviation of the input data\n",
        "print(\"Mean =\", Input_data.mean(axis=0))\n",
        "print(\"Stddeviation = \", Input_data.std(axis=0))\n",
        "#Removing the mean and the standard deviation of the input data\n",
        "\n",
        "data_scaled = preprocessing.scale(Input_data)\n",
        "print(\"Mean_removed =\", data_scaled.mean(axis=0))\n",
        "print(\"Stddeviation_removed =\", data_scaled.std(axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ3pvq70NEsT",
        "outputId": "d9910568-8954-42bf-9210-7a730ad85649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean = [ 1.75  -1.275  2.2  ]\n",
            "Stddeviation =  [2.71431391 4.20022321 4.69414529]\n",
            "Mean_removed = [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
            "Stddeviation_removed = [1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Scaling: We use this preprocessing technique for scaling the feature vectors. Scaling of feature vectors is important, because the features should not be synthetically large or small.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "Input_data = np.array(\n",
        "   [\n",
        "      [2.1, -1.9, 5.5],\n",
        "      [-1.5, 2.4, 3.5],\n",
        "      [0.5, -7.9, 5.6],\n",
        "      [5.9, 2.3, -5.8]\n",
        "   ]\n",
        ")\n",
        "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
        "data_scaled_minmax = data_scaler_minmax.fit_transform(Input_data)\n",
        "print (\"\\nMin max scaled data:\\n\", data_scaled_minmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ6-omItNeks",
        "outputId": "42475cb1-9e22-4ef3-8dee-91a6ae64519f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Min max scaled data:\n",
            " [[0.48648649 0.58252427 0.99122807]\n",
            " [0.         1.         0.81578947]\n",
            " [0.27027027 0.         1.        ]\n",
            " [1.         0.99029126 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Normalisation: We use this preprocessing technique for modifying the feature vectors. Normalisation of feature vectors is necessary so that the feature vectors can be measured at common scale. There are two types of normalisation as follows âˆ’\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "Input_data = np.array(\n",
        "   [\n",
        "      [2.1, -1.9, 5.5],\n",
        "      [-1.5, 2.4, 3.5],\n",
        "      [0.5, -7.9, 5.6],\n",
        "      [5.9, 2.3, -5.8]\n",
        "   ]\n",
        ")\n",
        "data_normalized_l1 = preprocessing.normalize(Input_data, norm='l1')\n",
        "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLhi--jdNaEZ",
        "outputId": "bf8fadd1-70ca-4574-8abe-cc025282b111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "L1 normalized data:\n",
            " [[ 0.22105263 -0.2         0.57894737]\n",
            " [-0.2027027   0.32432432  0.47297297]\n",
            " [ 0.03571429 -0.56428571  0.4       ]\n",
            " [ 0.42142857  0.16428571 -0.41428571]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "Input_data = np.array(\n",
        "   [\n",
        "      [2.1, -1.9, 5.5],\n",
        "      [-1.5, 2.4, 3.5],\n",
        "      [0.5, -7.9, 5.6],\n",
        "      [5.9, 2.3, -5.8]\n",
        "   ]\n",
        ")\n",
        "data_normalized_l2 = preprocessing.normalize(Input_data, norm='l2')\n",
        "print(\"\\nL1 normalized data:\\n\", data_normalized_l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD3MMCXmOJHs",
        "outputId": "092092a4-829f-4259-c69f-43f2e1c70bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "L1 normalized data:\n",
            " [[ 0.33946114 -0.30713151  0.88906489]\n",
            " [-0.33325106  0.53320169  0.7775858 ]\n",
            " [ 0.05156558 -0.81473612  0.57753446]\n",
            " [ 0.68706914  0.26784051 -0.6754239 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XxSNfA6OPmi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}